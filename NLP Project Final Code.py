# -*- coding: utf-8 -*-
"""NLP Final Project Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ck8T4gcnkDlfLJUrVK9aj-XS4_CVrLe7

# Initialization and Environment Setup
"""

!pip install pypdf --quiet
!pip install rank_bm25 --quiet
!pip install sentence-transformers --quiet

import os

import numpy

!pip install pymupdf --quiet
import fitz

from rank_bm25 import BM25Okapi as bm25fn
from pypdf import PdfReader as pypdf_reader
from re import sub as regex_sub

import torch

from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer


from nltk.corpus import stopwords
from nltk import download as nltk_download

nltk_download('punkt_tab'), nltk_download('stopwords')
ENGLISH_STOPWORDS = stopwords.words('english')

# Provide Search Queries Here
SEARCH_QUERIES = \
[
    string.lower().split() for string in \
    [
        "racial bias nurses",
        "minority health nurses bias",
        "class bias race healthcare",
        "weight loss",
        "tea powder weight loss",
        "berry health benefit",
    ]
]

# SEARCH_QUERIES = [ query.lower().split() for query in SEARCH_QUERIES ]
TARGET_RESULTS = [ "PaperA", "PaperA", "PaperA", "PaperB", "PaperB", "PaperC" ]

"""# Prepare Data
Separately generate cleaned and uncleaned sets of extracted pdf data
"""

datasets_path = "/content/drive/MyDrive/Dataset Corpus"
for file in os.scandir(datasets_path):
  if file.is_file() and os.path.splitext(file.path)[1] == ".pdf":

    file_text = ""

    pdf_reader = pypdf_reader(file.path)
    for pgidx in range(0, len(pdf_reader.pages)):
      file_text += pdf_reader.pages[pgidx].extract_text()

    with open(datasets_path + "/raw/" + os.path.splitext(file.name)[0] + ".raw.txt", "w") as outfile:
      outfile.write(file_text)

    with open(datasets_path + "/clean/" + os.path.splitext(file.name)[0] + ".clean.txt", "w") as outfile:
      cleaned_text = regex_sub("[0-9]", " ", regex_sub("[\\W]", " ", file_text.lower()))
      for word in cleaned_text.split():
        if word not in ENGLISH_STOPWORDS and len(word.strip()) != 0: outfile.write(word + " ")

    #with open(datasets_path + "/raw/" + os.splitext(file.name)[0] + ".raw.txt", "w") as outfile:
      #outfile.write(file_text)

"""# Unclean BM25"""

corpus, file_order = [ [], [] ]

datasets_path = "/content/drive/MyDrive/Dataset Corpus/raw"
for file in os.scandir(datasets_path):
  if file.is_file() and os.path.splitext(file.path)[1] == ".txt":
    with open(file.path, "r") as readfile:
      file_order.append(file.name.split('.')[0])
      corpus.append(readfile.read().split())


bm25, scores = [ bm25fn(corpus), [] ]

print(file_order, "\n")
for query in SEARCH_QUERIES:
  score = numpy.asarray(bm25.get_scores(query))

  score = numpy.exp(score) / numpy.sum(numpy.exp(score))
  scores.append(score)

  for s in score: print(f"{s:.4e}", end=",")
  print("\b;")
  # print("\n", file_order[score.argmax()], "\n")

"""# Clean BM25"""

corpus, file_order = [ [], [] ]

datasets_path = "/content/drive/MyDrive/Dataset Corpus/clean"
for file in os.scandir(datasets_path):
  if file.is_file() and os.path.splitext(file.path)[1] == ".txt":
    with open(file.path, "r") as readfile:
      file_order.append(file.name.split('.')[0])
      corpus.append(readfile.read().split())


bm25, scores = [ bm25fn(corpus), [] ]

print(file_order, "\n")
for query in SEARCH_QUERIES:
  score = numpy.asarray(bm25.get_scores(query))

  score = numpy.exp(score) / numpy.sum(numpy.exp(score))
  scores.append(score)

  for s in score: print(f"{s:e}", end=",")
  print("\b;")
  # print("\n", file_order[score.argmax()], "\n")

"""# Regular Sentence Modeler"""

datasets_path = "/content/drive/MyDrive/Dataset Corpus/clean"
corpus_documents = [
    open(file.path, "r").read() for file in os.scandir(datasets_path)
]

simple_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = simple_model.encode(corpus_documents)

embedded_queries = simple_model.encode(SEARCH_QUERIES)
scores = [
    numpy.dot(embedded_queries, document_embedding)
      / numpy.linalg.norm(embedded_queries) * numpy.linalg.norm(document_embedding)
    for document_embedding in embeddings
]

scores

"""# SciBERT Uncleaned


"""

datasets_path = "/content/drive/MyDrive/Dataset Corpus/raw"

access_order = []
corpus_documents = []
for file in os.scandir(datasets_path):
  access_order.append(file.name)
  corpus_documents.append(open(file.path, "r").read())

scibert_model = SentenceTransformer("pritamdeka/S-Scibert-snli-multinli-stsb")
document_embeddings = scibert_model.encode(corpus_documents)

query_embeddings = scibert_model.encode(SEARCH_QUERIES)

scores, results = [ [], [] ]
for qemb in query_embeddings:
  score = []
  for demb in document_embeddings:
    score.append(numpy.dot(qemb, demb) / (numpy.linalg.norm(qemb) * numpy.linalg.norm(demb)))
  scores.append(score)
  results.append(access_order[numpy.argmax(score)])


print("Target\t\tResult")
for i in range(len(scores)):
  # [ print(f"{s:.2E}", end="\t") for s in scores[i] ]
  # print()
  print(TARGET_RESULTS[i], "\t\t", results[i].split('.')[0])


#for score in scores:
#  print(numpy.exp(scores) / numpy.sum(numpy.exp(score)))

"""# SciBERT Cleaned"""

datasets_path = "/content/drive/MyDrive/Dataset Corpus/clean"

access_order = []
corpus_documents = []
for file in os.scandir(datasets_path):
  access_order.append(file.name)
  corpus_documents.append(open(file.path, "r").read())

scibert_model = SentenceTransformer("pritamdeka/S-Scibert-snli-multinli-stsb")
document_embeddings = scibert_model.encode(corpus_documents)

query_embeddings = scibert_model.encode(SEARCH_QUERIES)

scores, results = [ [], [] ]
for qemb in query_embeddings:
  score = []
  for demb in document_embeddings:
    score.append(numpy.dot(qemb, demb) / (numpy.linalg.norm(qemb) * numpy.linalg.norm(demb)))
  scores.append(score)
  results.append(access_order[numpy.argmax(score)])


print("Target\t\tResult")
for i in range(len(scores)):
  # [ print(f"{s:.2E}", end="\t") for s in scores[i] ]
  # print()
  print(TARGET_RESULTS[i], "\t\t", results[i].split('.')[0])